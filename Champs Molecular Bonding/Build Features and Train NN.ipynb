{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel is based on\n",
    "\n",
    "* https://www.kaggle.com/xwxw2929/keras-neural-net-and-distance-features\n",
    "* https://www.kaggle.com/todnewman/keras-neural-net-for-champs\n",
    "* https://www.kaggle.com/criskiev/distance-is-all-you-need-lb-1-481\n",
    "* https://www.kaggle.com/abazdyrev/nn-w-o-skew\n",
    "* https://www.kaggle.com/marcogorelli/criskiev-s-distances-more-estimators-groupkfold\n",
    "* https://www.kaggle.com/inversion/atomic-distance-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/timestocome/Kaggle\n",
    "### https://www.kaggle.com/c/champs-scalar-coupling\n",
    "\n",
    "### Goal: improve prediction of scalar coupling constant using only provided dataset\n",
    "\n",
    "####  Improvements\n",
    "* added plts of NN convergence\n",
    "* added plts of predicted vs actual in validation data\n",
    "* added functions to calculate VanDerWaals, Coulomb, Yukawa forces\n",
    "* made several changes to database manipulations to make them cleaner, faster\n",
    "*     and add in several features not in previous kernels\n",
    "* redid the neural network in the previous examples as they only smoothed out when overfitting the data\n",
    "\n",
    "\n",
    "* Most of my time in this competition was spent on feature engineering\n",
    "* There wasn't enough time at the end to get the ML NN fine tuned\n",
    "* Typically the data calculations would be done and stored in a csv so they didn't have to be\n",
    "*    recalculated each time the model was run --- but I ran out of time on this contest\n",
    "* Also I would've custom build a neural net for each coupling rather than use the same size one for all\n",
    "*     the data for each of the 8 coupling types varied from less than half a million rows to 1.5 million rows\n",
    "\n",
    "\n",
    "#### Fails\n",
    "* I read lots and lots of papers. I tried calculating all the nearest neighbors and angles using\n",
    "*     Networkx. Networkx was very slow and the angles turned out to be poor predictors\n",
    "\n",
    "* Karplus, Coulomb, Vanderwaals, Yukawa all yielded decent results but are heavily coupled confusing the \n",
    "*     neural network. Given more time I'd've tried all the combinations of each on each coupling type\n",
    "\n",
    "* Counting bonds and atoms were decent predictors but were also heavily coupled with distance and bond type\n",
    "\n",
    "* I also tried electonegativity, which was an okay predictor. Even in this last version some of the \n",
    "*     coupling types still have large std on test data despite the large size of the dataset\n",
    "\n",
    "* The dataset had several feature files for the training set only.  I tried using a two stage neural network\n",
    "*      to calculate them, then calculate the target but it wasn't very successful\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### New skills\n",
    "* The Pandas group-stack is very cool, it would've saved me weeks of slow feature calculations\n",
    "* Two stage neural networks - pull some outputs out before end layer and train to them as well as final \n",
    "*     target layer\n",
    "\n",
    "\n",
    "* Best possible score ~ -20\n",
    "* Winning score -3.2\n",
    "* My best -1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU preferences, needed for newer GPUs with Keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.Session(config=config) \n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in raw datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_train():\n",
    "    train = pd.read_csv('Data/train.csv', index_col=0)\n",
    "\n",
    "    # remove batch name from molecule id\n",
    "    train['molecule_index'] = train.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "    train.drop(['molecule_name'], axis=1, inplace=True)\n",
    "\n",
    "    coupling_types = sorted(train['type'].unique())\n",
    "\n",
    "    return train\n",
    "    \n",
    "train = read_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "\n",
    "    test = pd.read_csv('Data/test.csv', index_col=0)\n",
    "\n",
    "\n",
    "    # remove batch name from molecule id\n",
    "    test['molecule_index'] = test.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "    test.drop(['molecule_name'], axis=1, inplace=True)\n",
    "\n",
    "    return test\n",
    "    \n",
    "test = read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_struct():\n",
    "\n",
    "    struct = pd.read_csv('Data/structures.csv')\n",
    "\n",
    "    atomic_numbers = { 'H': 1, 'C':6, 'N':7, 'O':8, 'F': 9 }\n",
    "\n",
    "    # https://www.thoughtco.com/element-charges-chart-603986\n",
    "    atomic_chg = { 'H': 1, 'C': 4, 'N': -3, 'O': -2, 'F': -1 }\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Electronegativity\n",
    "    atomic_pchgs = { 'H': 2.2, 'C': 2.55, 'N': 3.04, 'O': 3.44, 'F': 3.98 }\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Atomic_radius\n",
    "    atomic_size = { 'H': 0.25, 'C': 0.7, 'N': 0.65, 'O': 0.60, 'F': 0.5 }\n",
    "\n",
    "\n",
    "    # get atom count per molecule and merge back into struct\n",
    "    u_atoms = struct.groupby(['molecule_name'])['atom'].value_counts()\n",
    "    u_atoms = u_atoms.unstack()\n",
    "    u_atoms = u_atoms.fillna(0)\n",
    "\n",
    "\n",
    "    # get OxBalance\n",
    "    u_atoms['total atoms'] = u_atoms['O'] + u_atoms['C'] + u_atoms['F'] + u_atoms['N'] + u_atoms['H']\n",
    "    u_atoms['Ox balance'] = (u_atoms['O'] - 2*u_atoms['C'] - u_atoms['H']) / u_atoms['total atoms']\n",
    "\n",
    "    struct = pd.merge(struct, u_atoms, how='left', left_on=['molecule_name'], right_on=['molecule_name'])\n",
    "\n",
    "\n",
    "\n",
    "    # remove batch name from molecule id\n",
    "    struct['molecule_index'] = struct.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "    struct.drop(['molecule_name'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    # swap out letter id for atoms with atomic_numbers\n",
    "    #struct['atom'] = struct['atom'].replace(atomic_size)   # -.17\n",
    "    #struct['atom'] = struct['atom'].replace(atomic_chg)   # -.25\n",
    "    #struct['atom'] = struct['atom'].replace(atomic_pchgs)   # -.22\n",
    "    struct['atom'] = struct['atom'].replace(atomic_numbers)   # -.25\n",
    "    return struct\n",
    "    \n",
    "struct = read_struct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('Data/sample_submission.csv', index_col='id')\n",
    "\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prep data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split out data types and merge with location db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pull coupling datatype from train/test and structures dfs\n",
    "def fetch_type_data(df, struct, ctype):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pull out all rows with this coupling type from train data and make a copy\n",
    "    df = df[df['type'] == ctype].drop('type', axis=1).copy()\n",
    "    \n",
    "    \n",
    "    # pull out all rows with this coupling type from structs\n",
    "    struct = read_struct()\n",
    "    struct_df = struct[struct['molecule_index'].isin(df['molecule_index'])]\n",
    "    \n",
    "    \n",
    "    return df, struct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge train/test with structs df\n",
    "def merge_coordinates(bonds, structures, index):\n",
    "    \n",
    "    df = pd.merge(bonds, structures, how='inner',\n",
    "                  left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    \n",
    "    if index == 0:\n",
    "        df.drop(columns=['C', 'F', 'H', 'N', 'O', 'total atoms', 'Ox balance'], inplace=True)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get distances between atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distance between each atom \n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    \n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coulomb force distance between each atom \n",
    "# 1/2 Z^2.4 if i!=j\n",
    "# Zi*Zj/|Ri - Rj|\n",
    "\n",
    "\n",
    "# atomic numbers should be used for atom\n",
    "def add_coulomb_between(df, suffix1, suffix2):\n",
    "    \n",
    "    \n",
    "    Zi = df[f'atom_{suffix1}']\n",
    "    Zj = df[f'atom_{suffix2}']\n",
    "    \n",
    "    #df[f'c_sm_{suffix1}_{suffix2}'] = 0.5 * Zi**2.4\n",
    "        \n",
    "    df[f'c_df{suffix1}_{suffix2}'] = (Zi * Zj) / ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "         )**np.float32(0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_coulombs(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_coulomb_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate  yukawa force distance between each atom \n",
    "\n",
    "def add_yuka_between(df, suffix1, suffix2):\n",
    "    \n",
    "    df[f'yk_{suffix1}_{suffix2}'] =  np.exp(-((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_yukas(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_yuka_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate vanderwaal's force distance between each atom \n",
    "\n",
    "def add_vander_between(df, suffix1, suffix2):\n",
    "    \n",
    "    df[f'v_{suffix1}_{suffix2}'] = 1./ ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_vanders(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_vander_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def build_df(df, structures, ctype, n_atoms):\n",
    " \n",
    "    \n",
    "    # fetch coupling type data from train/test and structs files\n",
    "    bonds, structs = fetch_type_data(df, struct, ctype)\n",
    "    \n",
    "    # merge structs and coupling data for atom col 0, 1\n",
    "    bonds = merge_coordinates(bonds, structs, 0)\n",
    "    bonds = merge_coordinates(bonds, structs, 1)\n",
    "   \n",
    "   \n",
    "    \n",
    "    # make a copy of df just built and drop target from training\n",
    "    atoms = bonds.copy()\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        atoms.drop(['scalar_coupling_constant'], axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "    # add center location between coupled atoms and drop xyz locations of each atom\n",
    "    atoms['x_c'] = ((atoms['x_1'] + atoms['x_0']) * np.float32(0.5))\n",
    "    atoms['y_c'] = ((atoms['y_1'] + atoms['y_0']) * np.float32(0.5))\n",
    "    atoms['z_c'] = ((atoms['z_1'] + atoms['z_0']) * np.float32(0.5))\n",
    "    \n",
    "    # add r\n",
    "    atoms['dx'] = ((atoms['x_1'] + atoms['x_0']) * np.float32(0.5))\n",
    "    atoms['dy'] = ((atoms['y_1'] + atoms['y_0']) * np.float32(0.5))\n",
    "    atoms['dz'] = ((atoms['z_1'] + atoms['z_0']) * np.float32(0.5))\n",
    "    atoms['r'] = np.sqrt(atoms['dx'] + atoms['dy'] + atoms['dz'])\n",
    "    atoms.drop(['dx', 'dy', 'dz'], axis=1, inplace=True)\n",
    "    \n",
    "    # drop location info\n",
    "    atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1, inplace=True)\n",
    "    \n",
    "    # merge location info back in\n",
    "    # this creates a row for each atom to all other atoms\n",
    "    atoms = pd.merge(atoms, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'], \n",
    "                    suffixes=('','_junk'))\n",
    "\n",
    "    # cleanup\n",
    "    atoms = atoms.drop([col for col in atoms.columns if '_junk' in col],axis=1)\n",
    "    atoms = atoms[(atoms.atom_index_0 != atoms.atom_index) & (atoms.atom_index_1 != atoms.atom_index)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # add distances for each atom to center of coupled pairs\n",
    "    atoms['d_c'] = ((\n",
    "        (atoms['x_c'] - atoms['x'])**np.float32(2) +\n",
    "        (atoms['y_c'] - atoms['y'])**np.float32(2) + \n",
    "        (atoms['z_c'] - atoms['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "    \n",
    "    \n",
    "    # cleanup\n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "\n",
    "\n",
    "    # sort by molecule, atom 0, atom 1 and distance to center of coupled pairs\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # group and unstack.... https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html\n",
    "    # group atoms by molecule and atoms\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # count the number of atoms in molecule by distance from coupled pair\n",
    "    # and remove distance to center --- only needed for sort\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    \n",
    "    # remove rows that are greater than the max of nearby atoms wanted for \n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "        \n",
    "   \n",
    "        \n",
    "    # convert molecule id, atom 0, atom 1 into index columns\n",
    "    # this adds cols for each atom up to n_atoms, and x, y, z for each atom that is kept\n",
    "    # relabel new columns and fix index\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "\n",
    "    \n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()  \n",
    "    \n",
    "     # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "\n",
    "    \n",
    "  \n",
    "  \n",
    "    # more cleanup\n",
    "    x_cols = [z for z in atoms.columns if z.startswith('x_')]\n",
    "    y_cols = [z for z in atoms.columns if z.startswith('y_')]\n",
    "    z_cols = [z for z in atoms.columns if z.startswith('z_')]\n",
    "    a_cols = [z for z in atoms.columns if z.startswith('atom_')]\n",
    "    atoms = atoms.rename( columns={'C_2': 'C', 'F_2': 'F', 'H_2': 'H', 'N_2': 'N', 'O_2': 'O', \n",
    "                    'total atoms_2': 'total atoms', 'Ox balance_2': 'Ox balance'})\n",
    "    \n",
    "    misc_features = ['molecule_index', #'atom_index_0', 'atom_index_1', \n",
    "                     'C', 'F', 'H', 'N', 'O', 'total atoms', 'Ox balance']\n",
    "    keep_cols = x_cols + y_cols + z_cols + a_cols + misc_features\n",
    "   \n",
    "    \n",
    "    atoms = atoms[keep_cols]\n",
    "     \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    # merge it all back together into coupling df\n",
    "    df = pd.merge(bonds, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'],\n",
    "                  suffixes=('','_junk'))\n",
    "    df = df.drop([col for col in df.columns if '_junk' in col],axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate distances from x,y,z of each atom and cleanup\n",
    "    add_distances(df)\n",
    "    add_coulombs(df)\n",
    "    add_yukas(df)\n",
    "    add_vanders(df)\n",
    "\n",
    "      \n",
    "    df = df.fillna(0)\n",
    "    df.drop(columns=['atom_index_0', 'atom_index_1'], inplace=True)\n",
    "    df.drop(['atom_0', 'atom_1'], axis=1, inplace=True)\n",
    "    \n",
    "    x_col0 = df.columns[df.columns.str.startswith('atom_0')]\n",
    "    x_col1 = df.columns[df.columns.str.startswith('atom_1')]\n",
    "    \n",
    "    df.drop(x_col0, axis=1, inplace=True)\n",
    "    df.drop(x_col1, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # bond energies\n",
    "   \n",
    "    \n",
    "    df['N/C'] = np.where(df['C'] > 0, df['N'] / df['C'], 0)\n",
    "    df['O/C'] = np.where(df['C'] > 0, df['O'] / df['C'], 0)\n",
    "    df['H/C'] = np.where(df['C'] > 0, df['H'] / df['C'], 0)\n",
    "    df['N-O'] = df['N'] - df['O']\n",
    "    df['C-N'] = df['C'] - df['N']\n",
    "    df['C-F'] = df['C'] - df['F']\n",
    "    df['C-N-H'] = df['C'] - df['N'] - df['H']\n",
    "    \n",
    "    df['C-N*O -C*N'] = df['C'] - df['N']*df['O'] - df['C']*df['N']\n",
    "    \n",
    "    \n",
    "   \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check predicted against actual - perfect score is line at 45'\n",
    "# scores alone don't contain enough information, \n",
    "#   eyeball predicted validation against actual validation\n",
    "#   can show trouble spots, outlier problems....\n",
    "\n",
    "def plot_output(actual, predicted, coupling_type):\n",
    "  \n",
    "    print('plotting output', actual.shape, predicted.shape, coupling_type)\n",
    "  \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.xlim(actual.min(), actual.max())\n",
    "    plt.ylim(actual.min(), actual.max())\n",
    "\n",
    "    plt.scatter(actual, predicted)\n",
    "    plt.title(coupling_type)\n",
    "    plt.grid(True)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart NN convergence progress\n",
    "def plot_history(history, label):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "    # remove first few points to take initial bounce off plots    \n",
    "    plt.plot(history.history['loss'][20:])\n",
    "    plt.plot(history.history['val_loss'][20:])\n",
    "    \n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_split_data(some_csv, coupling_type, n_atoms):\n",
    "    \n",
    "    df = build_df(some_csv, struct, coupling_type, n_atoms=n_atoms)\n",
    "        \n",
    "    molecule_index = df['molecule_index'].values\n",
    "    df.drop('molecule_index', axis=1, inplace=True)\n",
    "    \n",
    "         \n",
    "    # features\n",
    "    #atom_features = [z for z in df.columns if z.startswith('atom_')]\n",
    "    distance_features = [z for z in df.columns if z.startswith('d_')]\n",
    "    coulomb_features = [z for z in df.columns if z.startswith('c_')]\n",
    "    vanderwaal_features = [z for z in df.columns if z.startswith('v_')]\n",
    "    yuka_features = [z for z in df.columns if z.startswith('yk_')]\n",
    "    bond_features = ['N/C', 'O/C', 'H/C', 'N-O', 'C-N', 'C-F', 'C-N-H', 'C-N*O -C*N']\n",
    "    misc_features = ['C', 'F', 'H', 'N', 'O', 'total atoms', 'Ox balance']\n",
    "\n",
    "\n",
    "\n",
    "    features = coulomb_features + vanderwaal_features + yuka_features + misc_features + bond_features\n",
    "    #features = atom_features + distance_features\n",
    "    \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "        X_data = df[features].values.astype('float32')\n",
    "\n",
    "    else:\n",
    "        X_data = df[features].values.astype('float32')\n",
    "        y_data = None\n",
    "       \n",
    "    \n",
    "    X_data = StandardScaler().fit_transform(X_data)\n",
    "    print(X_data.shape)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN\n",
    "def create_nn_model(input_shape):\n",
    "    \n",
    "    inp = Input(shape=(input_shape,))\n",
    "    \n",
    "    x = Dense(2048, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    x = Dense(2048, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    \n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    \n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "    \n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set up NN\n",
    "mol_types = sorted(train[\"type\"].unique())\n",
    "cv_score = []\n",
    "cv_score_total = 0\n",
    "epoch_n = 200\n",
    "verbose = 1\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =True\n",
    "\n",
    "# check time per run\n",
    "start_time = datetime.now()\n",
    "\n",
    "# set up submission file\n",
    "test_prediction = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# train a different model for each coupling type\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "\n",
    "    # use to save best model for each coupling type\n",
    "    model_name_wrt = ('molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    #################################################################################\n",
    "    # data prep\n",
    "    print(f'*** Training Model for {mol_type} ***')\n",
    "    \n",
    "    \n",
    "    if mol_type == '1JHC': n_atoms = 14\n",
    "    elif mol_type == '1JHN': n_atoms = 7\n",
    "    elif mol_type == '2JHC': n_atoms = 14\n",
    "    elif mol_type == '2JHH': n_atoms = 12\n",
    "    elif mol_type == '2JHN': n_atoms = 12\n",
    "    elif mol_type == '3JHC': n_atoms = 14\n",
    "    elif mol_type == '3JHH': n_atoms = 9\n",
    "    elif mol_type == '3JHN': n_atoms = 12\n",
    "\n",
    "    if mol_type == '1JHC': stop_dx = 0.00001\n",
    "    elif mol_type == '1JHN': stop_dx = 0.0001\n",
    "    elif mol_type == '2JHC': stop_dx = 0.00001\n",
    "    elif mol_type == '2JHH': stop_dx = 0.00001\n",
    "    elif mol_type == '2JHN': stop_dx = 0.00001\n",
    "    elif mol_type == '3JHC': stop_dx = 0.00001\n",
    "    elif mol_type == '3JHH': stop_dx = 0.0001\n",
    "    elif mol_type == '3JHN': stop_dx = 0.001\n",
    "\n",
    "    # create dataset from input files and split  into train/test \n",
    "    \n",
    "    X_train, y_train = build_and_split_data(train, mol_type, n_atoms)\n",
    "    test_input, _ = build_and_split_data(test, mol_type, n_atoms)\n",
    "    \n",
    "       \n",
    "    \n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    # set random number for same split, better to shuffle and run different \n",
    "    # might show weirdenesses otherwise hidden by unfortunate split\n",
    "    train_index, cv_index = train_test_split(np.arange(len(X_train)), test_size=0.1)\n",
    "    \n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_target = y_train[train_index]\n",
    "    cv_target = y_train[cv_index]\n",
    "    \n",
    "    train_input = X_train[train_index]\n",
    "    cv_input = X_train[cv_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #test_input = input_data[len(df_train_):,:]\n",
    "    #y_pred = np.zeros(train_target.shape[0], dtype='float32')\n",
    "\n",
    "    ################################################################################\n",
    "    # Build the Neural Net \n",
    "    nn_model = create_nn_model(train_input.shape[1])\n",
    "   \n",
    "        \n",
    "        \n",
    "    # compile model    \n",
    "    nn_model.compile(loss='mse', optimizer=Adam())   \n",
    "\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping( monitor = 'val_loss', \n",
    "                                 min_delta = stop_dx, \n",
    "                                 patience = 30, \n",
    "                                 verbose = 1, \n",
    "                                 mode = 'auto', \n",
    "                                 restore_best_weights = True)\n",
    "    \n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau( monitor = 'val_loss',\n",
    "                                      factor = 0.5, \n",
    "                                      patience = 20, \n",
    "                                      min_lr = 1e-6, \n",
    "                                      mode = 'auto', \n",
    "                                      verbose = 1)\n",
    "    \n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "    \n",
    "    ################################################################################\n",
    "    # train network\n",
    "    # save convergence history for plots - can use to check overfitting\n",
    "    history = nn_model.fit(train_input, [train_target], \n",
    "                                validation_data = (cv_input, [cv_target]), \n",
    "                                callbacks = [rlr, sv_mod], \n",
    "                                epochs = epoch_n, \n",
    "                                shuffle = True,\n",
    "                                batch_size = batch_size, \n",
    "                                verbose = verbose)\n",
    "    \n",
    "    ################################################################################\n",
    "    # check validation data\n",
    "    cv_predict = nn_model.predict(cv_input)\n",
    "    plot_output(cv_predict, cv_target, mol_type)\n",
    "    \n",
    "    # plot convergence\n",
    "    plot_history(history, mol_type)\n",
    "    accuracy = np.mean(np.abs(cv_target-cv_predict[:,0]))\n",
    "    print(np.log(accuracy))\n",
    "    \n",
    "    # save scores\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total += np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict = nn_model.predict(test_input)\n",
    "    \n",
    "    ################################################################################\n",
    "    # for each molecule type grab the predicted values\n",
    "    test_prediction[test[\"type\"] == mol_type] = test_predict[:,0]\n",
    "    \n",
    "    # reset\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total /= len(mol_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prep submission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scores\n",
    "\n",
    "for i in range(len(mol_types)):\n",
    "    print(mol_types[i], cv_score[i])\n",
    "    \n",
    "print('Final ', sum(cv_score) / len(mol_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "\n",
    "def submits(predictions):\n",
    "\n",
    "    submission['scalar_coupling_constant'] = predictions\n",
    "    print(submission.head(30))\n",
    "\n",
    "    submission.to_csv('nn_submission_3.csv')\n",
    "    \n",
    "submits(test_prediction) \n",
    "    \n",
    "print('finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
