

# Linda MacPhee-Cobb
# https://github.com/timestocome

# MS Malware Detection
# https://www.kaggle.com/c/microsoft-malware-prediction


# read in raw data and digitize categories for use in machine learning algorithms


import numpy as np
import pandas as pd


pd.set_option('display.max_columns', 400)
pd.set_option('display.max_rows', 1000)
pd.set_option('precision', 5)
np.set_printoptions(precision=5)


#################################################################################################
# used sed to cleanup stuff pandas choked on
# sed -i 's/,59e3,'/,-1,/g' train.csv



#################################################################################################
# read in raw data and do some cleanup
# reading everything in as an object gives unique count and most common feature count
# need to keep test and train in sync with same category ids matching between data sets
# String them together then split them back apart before saving

#################################################################################################
print('reading in training data')
train0 = pd.read_csv('sorted_train.csv', index_col=0)
train1 = pd.read_csv('sorted_test.csv', index_col=0)

train0['dummy'] = 0
train1['dummy'] = 1


print('reading in test data')
test = pd.read_csv('test.csv', index_col=0)

print('concating data')
data = pd.concat([train0, train1, test])

print('train %d, test %d, submit %d' %(len(train0), len(train1), len(test)))


features = list(data.columns.values)



# misc adjustments
data.fillna(-1, inplace=True)

print(data.head())
print(data.dtypes)


# split out objects
features_object = list(data.select_dtypes(include='object').columns)
features_int = list(data.select_dtypes(include='int64').columns)
features_float = list(data.select_dtypes(include='float64').columns)



features = list(data.columns.values)
n_features = len(features)


#features.remove('HasDetections')
print(features)
print(n_features)



###########################################################################################################
# a bit of cleanup
##########################################################################################################


# look for features with a large number of categories
for f in features:
    print(f)
    print(data[f].unique())



# reduce lots of categories down to a reasonable level
bins = [0, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]

data['Census_SystemVolumeTotalCapacity'] = pd.to_numeric(data['Census_SystemVolumeTotalCapacity'])
data['Census_PrimaryDiskTotalCapacity'] = pd.to_numeric(data['Census_PrimaryDiskTotalCapacity'])
data['Census_InternalBatteryNumberOfCharges'] = pd.to_numeric(data['Census_InternalBatteryNumberOfCharges'])
data['Census_TotalPhysicalRAM'] = pd.to_numeric(data['Census_TotalPhysicalRAM'])


data['C_PrimaryDiskTotalCapacity'] = pd.cut(data['Census_PrimaryDiskTotalCapacity'], bins)
data['C_SystemVolumeTotalCapacity'] = pd.cut(data['Census_SystemVolumeTotalCapacity'], bins)
data['C_InternalBatteryNumberOfCharges'] = pd.cut(data['Census_InternalBatteryNumberOfCharges'], bins)
data['C_TotalPhysicalRAM'] = pd.cut(data['Census_TotalPhysicalRAM'], bins)


data['C_PrimaryDiskTotalCapacity'] = pd.cut(data['Census_PrimaryDiskTotalCapacity'], bins)
data['C_SystemVolumeTotalCapacity'] = pd.cut(data['Census_SystemVolumeTotalCapacity'], bins)
data['C_InternalBatteryNumberOfCharges'] = pd.cut(data['Census_InternalBatteryNumberOfCharges'], bins)
data['C_TotalPhysicalRAM'] = pd.cut(data['Census_TotalPhysicalRAM'], bins)


data.drop('Census_PrimaryDiskTotalCapacity', axis=1, inplace=True)
data.drop('Census_SystemVolumeTotalCapacity', axis=1, inplace=True)
data.drop('Census_InternalBatteryNumberOfCharges', axis=1, inplace=True)
data.drop('Census_TotalPhysicalRAM', axis=1, inplace=True)




labels, uniques = pd.factorize(data['C_PrimaryDiskTotalCapacity'])
data['C_PrimaryDiskTotalCapacity'] = labels

labels, uniques = pd.factorize(data['C_SystemVolumeTotalCapacity'])
data['C_SystemVolumeTotalCapacity'] = labels

labels, uniques = pd.factorize(data['C_InternalBatteryNumberOfCharges'])
data['C_InternalBatteryNumberOfCharges'] = labels

labels, uniques = pd.factorize(data['C_TotalPhysicalRAM'])
data['C_TotalPhysicalRAM'] = labels





# get list of unique values in each column and assign an int to each
for f in features_object:
    print(f)
    data[f] = data[f].str.lower()
    labels, uniques = pd.factorize(data[f])
    data[f] = labels
    print('uniques', len(uniques))



print(data.head())
print(data.isnull().any(0))


###################################################################################################################
# write to disk
###################################################################################################################

print('splitting data back into two parts')

# split datasets back apart
train0 = data[data['HasDetections'] == 0]
train1 = data[data['HasDetections'] == 1]

train = pd.concat([train0, train1])



# split datasets apart
s_train0 = train[train['dummy'] == 0]
s_train1 = train[train['dummy'] == 1]

s_train0.drop('dummy', axis=1, inplace=True)
s_train1.drop('dummy', axis=1, inplace=True)


s_train0.to_csv('clean_train.csv')
s_train1.to_csv('clean_test.csv')




test = data[data['HasDetections'] == -1]
test.drop('HasDetections', axis=1,  inplace=True)


print('train %d, test %d submit %d' %(len(s_train0), len(s_train1), len(test)))

test.to_csv('clean_submit.csv')


print('finished')




